<div>
<font color=Blue size=6>Chapter 3 Automatic Differentiation</font> 
</div>

自动微分（Automatic Differentiation，简称 AD）是一种计算函数导数的技术，它在机器学习、优化和其他科学计算领域中非常重要。与手动微分和符号微分相比，自动微分更加高效和准确。
自动微分有两种主要形式：前向模式（Forward Mode）和反向模式（Reverse Mode）。这两种模式在计算图（Computation Graph）上的应用有所不同。
### 前向模式自动微分
在前向模式自动微分中，**计算从输入开始，沿着计算图向前传播，直到输出**。在这个过程中，每个变量的导数（也称为“种子”）都会被计算并存储起来。前向模式自动微分适用于函数的**输入维度远小于输出维度**的情况。
例如，考虑一个函数 \( f(x, y) = x^2 + xy \)，其中 \( x = 2 \) 和 \( y = 3 \)。在前向模式自动微分中，我们首先计算 \( x \) 和 \( y \) 的导数（种子），然后沿着计算图向前传播这些导数。
1. \( \frac{df}{dx} = 2x + y = 2 \times 2 + 3 = 7 \)
2. \( \frac{df}{dy} = x = 2 \)
### 反向模式自动微分
反向模式自动微分是计算梯度（即函数对所有输入变量的偏导数）的常用方法。在反向模式中，计算**从输出开始，沿着计算图反向传播，直到输入**。在这个过程中，每个变量的梯度都会被计算并存储起来。反向模式自动微分适用于函数的**输出维度远小于输入维度**的情况。
继续上面的例子，我们计算 \( f(x, y) = x^2 + xy \) 的梯度。
1. \( \frac{df}{dx} = 2x + y = 2 \times 2 + 3 = 7 \)
2. \( \frac{df}{dy} = x = 2 \)
因此，梯度是 \( (7, 2) \)。
### 自动微分的优点
1. **效率**：自动微分可以避免手动微分中的错误和繁琐，同时比符号微分更高效。
2. **灵活性**：自动微分可以处理复杂的函数和模型，包括嵌套和递归函数。
3. **精确性**：自动微分可以提供机器精度级别的导数。
### 自动微分的实现
自动微分可以通过以下几种方式实现：
1. **源代码转换**：在源代码级别上应用自动微分规则。
2. **操作符重载**：在数值类型上重载数学操作符，以便在运行时收集计算图。
3. **图分解**：构建计算图，然后使用图分解算法来计算导数。
在 Python 中，自动微分可以通过库如 Autograd、TensorFlow、PyTorch 和 JAX 实现。这些库提供了自动微分的功能，使得在机器学习和优化任务中计算梯度变得非常简单。
例如，在 JAX 中，你可以使用 `grad` 函数来计算函数的梯度：
```python
import jax
import jax.numpy as jnp
def f(x):
    return jnp.sin(x) ** 2
x = jnp.linspace(0, 10, 100)
df_dx = jax.grad(f)(x)
print(df_dx)
```
这将计算函数 \( f(x) = \sin(x)^2 \) 在 \( x \) 处的梯度。
